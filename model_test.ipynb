{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f5c7fd6",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af8ce76",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# Data\n",
    "ROOT_DIR = Path(\"/home/work/djyoon/project/cot-data-validation\")\n",
    "DATA_DIR = ROOT_DIR / \"data\"\n",
    "OUTPUT_DIR = ROOT_DIR / \"output\"\n",
    "VERSION = \"final_251120\"\n",
    "INPUT_DATA_PATH = DATA_DIR / \"가공데이터\"\n",
    "OUTPUT_DATA_PATH = DATA_DIR / VERSION\n",
    "\n",
    "# Training\n",
    "# BASE_MODEL = \"google/gemma-3-1b-it\"\n",
    "# TOKENIZER_MODEL = BASE_MODEL\n",
    "# BASE_MODEL_NAME = BASE_MODEL.split(\"/\")[1]\n",
    "# EXP_NAME = f\"cot/{BASE_MODEL_NAME}-{VERSION}\"\n",
    "# OUTPUT_MODEL_PATH = OUTPUT_DIR / \"models\" / EXP_NAME\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee949ecb",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2dc1dd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from os.path import exists\n",
    "\n",
    "from datasets import Dataset, DatasetDict, load_from_disk\n",
    "\n",
    "\n",
    "SPECIAL_ANS = \"answer\"\n",
    "TEST_SIZE = 0.1  # train:val:test = 8:1:1\n",
    "\n",
    "\n",
    "def generate_dataset(\n",
    "    input_path: str | Path, output_path: str | Path, test_size: float = TEST_SIZE\n",
    ") -> DatasetDict:\n",
    "    \"\"\"Generate dataset from source data path\"\"\"\n",
    "    if exists(output_path):\n",
    "        return load_from_disk(output_path)\n",
    "    else:\n",
    "        random.seed(SEED)\n",
    "        random.shuffle(data)\n",
    "\n",
    "        data = load_json(input_path)\n",
    "        n = len(data)\n",
    "        ds = DatasetDict(\n",
    "            {\n",
    "                \"train\": Dataset.from_list(data[int(n * 2 * test_size) :]),\n",
    "                \"val\": Dataset.from_list(\n",
    "                    data[int(n * test_size) : int(n * 2 * test_size)]\n",
    "                ),\n",
    "                \"test\": Dataset.from_list(data[: int(n * test_size)]),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Generate qa set\n",
    "        for id in ds:\n",
    "            ds[id] = ds[id].map(\n",
    "                generate_pair, remove_columns=[\"instruction\", \"context\", \"target_steps\"]\n",
    "            )\n",
    "\n",
    "        ds.save_to_disk(output_path)\n",
    "        print(ds)\n",
    "        print(f\"Dataset is saved to {output_path}.\")\n",
    "\n",
    "    return ds\n",
    "\n",
    "\n",
    "def load_json(base_path: Path | str) -> list[dict]:\n",
    "    \"\"\"base_path 하위 모든 폴더를 재귀 탐색하여 JSON 파일 로드\"\"\"\n",
    "    json_files = glob(f\"{base_path}/**/*.json\", recursive=True)\n",
    "    data = []\n",
    "    for path in json_files:\n",
    "        try:\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                obj = json.load(f)\n",
    "            if isinstance(obj, list):\n",
    "                data.extend([build_record(o) for o in obj])\n",
    "            else:\n",
    "                data.append(build_record(obj))\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ {f} 로드 실패: {e}\")\n",
    "    print(f\"✅ 총 {len(data)}개 JSON 파일 로드 완료 (경로: {base_path})\")\n",
    "    return data\n",
    "\n",
    "\n",
    "def build_record(obj):\n",
    "    \"\"\"단일 JSON 객체 → CoT 학습용 포맷 (가치 산출 중심, auto context generation)\"\"\"\n",
    "    p = obj.get(\"patent_info\", {})\n",
    "    c = obj.get(\"Company_info\", {})\n",
    "    ins = obj.get(\"instruction_id\", {})\n",
    "    val = obj.get(\"valuation_id\", {})\n",
    "\n",
    "    # ----- Instruction -----\n",
    "    instruction = (\n",
    "        ins.get(\"input\") or f\"{ins.get('title_ko','')} 특허의 가치를 단계별로 계산하라.\"\n",
    "    )\n",
    "\n",
    "    # ----- Dynamic Context Builder -----\n",
    "    ctx_parts = []\n",
    "\n",
    "    # 1. 특허 기본 정보\n",
    "    for key, label in [\n",
    "        (\"invention_title\", \"특허명\"),\n",
    "        (\"application_number\", \"출원번호\"),\n",
    "        (\"open_number\", \"공개번호\"),\n",
    "        (\"register_number\", \"등록번호\"),\n",
    "        (\"ipc_all\", \"IPC\"),\n",
    "        (\"applicant_name\", \"출원인\"),\n",
    "    ]:\n",
    "        val_ = p.get(key)\n",
    "        if val_:\n",
    "            ctx_parts.append(f\"{label}: {val_}\")\n",
    "\n",
    "    # 2. 회사/산업 정보\n",
    "    for key, label in [\n",
    "        (\"company_name\", \"회사명\"),\n",
    "        (\"industry\", \"산업분류\"),\n",
    "        (\"ksic\", \"KSIC 코드\"),\n",
    "        (\"sales\", \"매출액\"),\n",
    "        (\"net_income\", \"당기순이익\"),\n",
    "        (\"asset\", \"총자산\"),\n",
    "        (\"liabilities\", \"부채\"),\n",
    "        (\"equity\", \"자본\"),\n",
    "    ]:\n",
    "        val_ = c.get(key)\n",
    "        if val_ not in [None, \"\", 0]:\n",
    "            ctx_parts.append(f\"{label}: {val_}\")\n",
    "\n",
    "    # 3. 가치평가 관련 파라미터\n",
    "    for key, label in [\n",
    "        (\"royalty_rate\", \"로열티율(%)\"),\n",
    "        (\"useful_life_years\", \"경제적 수명(년)\"),\n",
    "        (\"wacc\", \"WACC(%)\"),\n",
    "        (\"business_risk\", \"사업위험도\"),\n",
    "    ]:\n",
    "        val_ = val.get(key, ins.get(key))\n",
    "        if val_ not in [None, \"\", 0]:\n",
    "            ctx_parts.append(f\"{label}: {val_}\")\n",
    "\n",
    "    # 4. 키워드 및 요약\n",
    "    if ins.get(\"keywords\"):\n",
    "        ctx_parts.append(f\"핵심 키워드: {', '.join(ins['keywords'])}\")\n",
    "    if ins.get(\"abstract_ko\"):\n",
    "        ctx_parts.append(f\"요약: {ins['abstract_ko'][:300]}...\")\n",
    "\n",
    "    # 5. 목표\n",
    "    ctx_parts.append(\n",
    "        \"이 데이터를 기반으로 로열티 공제법(Royalty Relief Method)을 사용하여 특허의 경제적 가치를 계산하라.\"\n",
    "    )\n",
    "\n",
    "    # ----- Target Steps & Answer -----\n",
    "    steps = \"\\n\".join(ins.get(\"output\", []))\n",
    "    ans = ins.get(\"answer\")\n",
    "\n",
    "    return {\n",
    "        \"instruction\": instruction.strip(),\n",
    "        \"context\": \"\\n\".join(ctx_parts).strip(),\n",
    "        \"target_steps\": steps.strip(),\n",
    "        \"target_answer\": ans,\n",
    "    }\n",
    "\n",
    "\n",
    "def generate_pair(d: dict) -> dict:\n",
    "    question = (\n",
    "        f\"당신은 특허 가치평가 전문가입니다.\\n\"\n",
    "        f\"주어진 데이터를 바탕으로 로열티 공제법(Royalty Relief Method)을 사용하여 특허의 경제적 가치를 추정하세요.\\n\"\n",
    "        f\"모든 계산은 단계별로 명확히 제시하고, 각 단계마다 수식과 계산 근거를 설명하세요.\\n\"\n",
    "        f\"최종 단계에서는 할인 후 현재가치를 계산하여 결과를 제시합니다.\\n\\n\"\n",
    "        f\"[INSTRUCTION]\\n{d['instruction']}\\n\\n\"\n",
    "        f\"[CONTEXT]\\n{d.get('context','없음')}\\n\\n\"\n",
    "        f\"[OUTPUT FORMAT]\\n\"\n",
    "        f\"1. 단계별 계산 과정 (예: 매출 추정 → 로열티율 적용 → 세후 조정 → 할인 계산)\\n\"\n",
    "        f'2. 마지막 줄에는 반드시 \"<{SPECIAL_ANS}>{{정수}}원</{SPECIAL_ANS}>\" 형식으로 최종 특허 가치를 표시하세요.\\n\\n'\n",
    "        f\"[EXAMPLE]\\n\"\n",
    "        \"① ...\\n\"\n",
    "        \"② ...\\n\"\n",
    "        \"...\\n\\n\"\n",
    "        f\"<{SPECIAL_ANS}>152,000,000원</{SPECIAL_ANS}>\"\n",
    "    )\n",
    "    steps = d.get(\"target_steps\", \"\")\n",
    "    ans = d.get(\"target_answer\", None)\n",
    "    if ans is None:\n",
    "        answer = steps\n",
    "    else:\n",
    "        answer = f\"{steps}\\n\\n<{SPECIAL_ANS}>{int(ans):,}원</{SPECIAL_ANS}>\"\n",
    "    return {\"question\": question, \"answer\": answer}\n",
    "\n",
    "\n",
    "def combine_datasets(\n",
    "    ds1: DatasetDict, ds2: DatasetDict, ds_dir: str, version: str\n",
    ") -> DatasetDict:\n",
    "    \"\"\"두 DatasetDict를 합쳐서 새로운 DatasetDict 생성\"\"\"\n",
    "    combined = {}\n",
    "    for split in [\"train\", \"val\", \"test\"]:\n",
    "        combined_data = ds1[split].to_list() + ds2[split].to_list()\n",
    "        combined[split] = Dataset.from_list(combined_data)\n",
    "    ds = DatasetDict(combined)\n",
    "\n",
    "    # Save to disk\n",
    "    ds.save_to_disk(ds_dir / version)\n",
    "    print(\n",
    "        f\"Data length: train({len(ds['train'])}), val({len(ds['val'])}), test({len(ds['test'])})\"\n",
    "    )\n",
    "    return ds\n",
    "\n",
    "\n",
    "dataset = generate_dataset(INPUT_DATA_PATH, OUTPUT_DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd578dda",
   "metadata": {},
   "source": [
    "### Serve trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30947e3b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "```bash\n",
    "vllm serve \"output/models/unsloth/cot/gemma-3-1b-it-final_251120\" \\\n",
    "    --gpu-memory-utilization 0.8 \\\n",
    "    --max-num-seqs 512 \\\n",
    "    --enable-chunked-prefill \\\n",
    "    --port 8010\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce68b09",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed01fc5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from time import time\n",
    "from os.path import exists\n",
    "\n",
    "import os\n",
    "import asyncio\n",
    "\n",
    "import pandas as pd\n",
    "from openai import AsyncOpenAI\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "\n",
    "\n",
    "def extract_answer(txt: str, marker: str = SPECIAL_ANS) -> int | None:\n",
    "    # NOTE: Use findall to capture the last occurrence, avoiding examples in the prompt\n",
    "    matches = re.findall(rf\"<{marker}>([-+]?[\\d]+[\\d.,]*)원?</{marker}>\", txt)\n",
    "    if not matches:\n",
    "        return None\n",
    "    try:\n",
    "        m_str = matches[-1]\n",
    "        num = re.sub(r\"(,|\\.{2,})\", \"\", m_str)\n",
    "        return round(float(num))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "async def evaluate_results(\n",
    "    ds, result_path: str, url: str, max_new_tokens: int = 2048\n",
    ") -> pd.DataFrame:\n",
    "    if exists(result_path):\n",
    "        return pd.read_csv(result_path)\n",
    "\n",
    "    # Generate predictions\n",
    "    questions = list(ds[\"question\"])\n",
    "\n",
    "    client = AsyncOpenAI(base_url=f\"{url}/v1\", api_key=\"empty\")\n",
    "    semaphore = asyncio.Semaphore(256)\n",
    "\n",
    "    model = (await client.models.list()).data[0].id\n",
    "\n",
    "    async def safe_create(prompt: str) -> str:\n",
    "        async with semaphore:\n",
    "            try:\n",
    "                response = await client.chat.completions.create(\n",
    "                    model=model,\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                    max_tokens=max_new_tokens,\n",
    "                )\n",
    "            except Exception as e:\n",
    "                response = \"\"\n",
    "            return response\n",
    "\n",
    "    tasks = [safe_create(prompt) for prompt in questions]\n",
    "    responses = await tqdm_asyncio.gather(*tasks)\n",
    "\n",
    "    # Analyze results\n",
    "    answers = [c.choices[0].message.content for c in responses]\n",
    "    target_answers_pred = [extract_answer(s) for s in answers]\n",
    "    target_answers = list(ds[\"target_answer\"])\n",
    "    diffs = [\n",
    "        (abs(p - l) / l) if p and l else None\n",
    "        for p, l in zip(target_answers_pred, target_answers)\n",
    "    ]\n",
    "    result = {\n",
    "        \"question\": questions,\n",
    "        \"target_answer_pred\": target_answers_pred,\n",
    "        \"target_answer\": target_answers,\n",
    "        \"answer_pred\": answers,\n",
    "        \"answer\": list(ds[\"answer\"]),\n",
    "        \"diff\": diffs,\n",
    "        \"accuracy\": [diff <= 0.3 if diff is not None else None for diff in diffs],\n",
    "    }\n",
    "    df = pd.DataFrame(result)\n",
    "    df.to_csv(result_path, index=False)\n",
    "    print(f\"Result has been successfully saved to {result_path}.\")\n",
    "    return df\n",
    "\n",
    "\n",
    "eval_dir = OUTPUT_DIR / \"evaluation\" / \"unsloth/cot/gemma-3-1b-it-final_251120\"\n",
    "os.makedirs(eval_dir, exist_ok=True)\n",
    "\n",
    "tasks = []\n",
    "data_id = \"test\"\n",
    "model_id = \"final_251120\"\n",
    "url = \"http://localhost:8010\"\n",
    "result_path = f\"{eval_dir}/{model_id}-{data_id}.csv\"\n",
    "tasks.append(evaluate_results(dataset[data_id], result_path, url))\n",
    "\n",
    "result = await asyncio.gather(*tasks)\n",
    "\n",
    "\n",
    "for path in sorted(glob(f\"{eval_dir}/*-test.csv\")):\n",
    "    df = pd.read_csv(path)\n",
    "    print(f\"MAP({path}): {df['diff'].mean():.2%}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
